
Qwen2MoeModel.__init__
Args: ['Qwen2MoeModel', 'Qwen2MoeConfig {\n  "_name_or_path": "/home/chiarolrg/work/Qwen2-57B-A14B-Instruct",\n  "architectures": [\n    "Qwen2MoeForCausalLM"\n  ],\n  "attention_dropout": 0.0,\n  "bos_token_id": 151643,\n  "decoder_sparse_step": 1,\n  "eos_token_id": 151643,\n  "hidden_act": "silu",\n  "hidden_size": 3584,\n  "initializer_range": 0.02,\n  "intermediate_size": 18944,\n  "max_position_embeddings": 32768,\n  "max_window_layers": 28,\n  "mlp_only_layers": [],\n  "model_type": "qwen2_moe",\n  "moe_intermediate_size": 2560,\n  "norm_topk_prob": false,\n  "num_attention_heads": 28,\n  "num_experts": 64,\n  "num_experts_per_tok": 8,\n  "num_hidden_layers": 28,\n  "num_key_value_heads": 4,\n  "output_router_logits": false,\n  "rms_norm_eps": 1e-06,\n  "rope_theta": 1000000.0,\n  "router_aux_loss_coef": 0.001,\n  "shared_expert_intermediate_size": 20480,\n  "sliding_window": null,\n  "tie_word_embeddings": false,\n  "torch_dtype": "bfloat16",\n  "transformers_version": "4.43.2",\n  "use_cache": true,\n  "use_sliding_window": false,\n  "vocab_size": 151936\n}\n']
Kwargs: {}

KQwen2MoeModel.__init__
Args: ['KQwen2MoeModel']
Kwargs: {'key': 'model', 'gguf_loader': '<ktransformers.util.custom_gguf.GGUFLoader object at 0x7f5b2178a010>', 'config': 'Qwen2MoeConfig {\n  "_name_or_path": "/home/chiarolrg/work/Qwen2-57B-A14B-Instruct",\n  "architectures": [\n    "Qwen2MoeForCausalLM"\n  ],\n  "attention_dropout": 0.0,\n  "bos_token_id": 151643,\n  "decoder_sparse_step": 1,\n  "eos_token_id": 151643,\n  "hidden_act": "silu",\n  "hidden_size": 3584,\n  "initializer_range": 0.02,\n  "intermediate_size": 18944,\n  "max_position_embeddings": 32768,\n  "max_window_layers": 28,\n  "mlp_only_layers": [],\n  "model_type": "qwen2_moe",\n  "moe_intermediate_size": 2560,\n  "norm_topk_prob": false,\n  "num_attention_heads": 28,\n  "num_experts": 64,\n  "num_experts_per_tok": 8,\n  "num_hidden_layers": 28,\n  "num_key_value_heads": 4,\n  "output_router_logits": false,\n  "rms_norm_eps": 1e-06,\n  "rope_theta": 1000000.0,\n  "router_aux_loss_coef": 0.001,\n  "shared_expert_intermediate_size": 20480,\n  "sliding_window": null,\n  "tie_word_embeddings": false,\n  "torch_dtype": "bfloat16",\n  "transformers_version": "4.43.2",\n  "use_cache": true,\n  "use_sliding_window": false,\n  "vocab_size": 151936\n}\n', 'orig_module': 'Qwen2MoeModel(\n  (embed_tokens): Embedding(151936, 3584)\n  (layers): ModuleList(\n    (0-27): 28 x Qwen2MoeDecoderLayer(\n      (self_attn): Qwen2MoeFlashAttention2(\n        (q_proj): Linear(in_features=3584, out_features=3584, bias=True)\n        (k_proj): Linear(in_features=3584, out_features=512, bias=True)\n        (v_proj): Linear(in_features=3584, out_features=512, bias=True)\n        (o_proj): Linear(in_features=3584, out_features=3584, bias=False)\n        (rotary_emb): Qwen2MoeRotaryEmbedding()\n      )\n      (mlp): Qwen2MoeSparseMoeBlock(\n        (gate): Linear(in_features=3584, out_features=64, bias=False)\n        (experts): ModuleList(\n          (0-63): 64 x Qwen2MoeMLP(\n            (gate_proj): Linear(in_features=3584, out_features=2560, bias=False)\n            (up_proj): Linear(in_features=3584, out_features=2560, bias=False)\n            (down_proj): Linear(in_features=2560, out_features=3584, bias=False)\n            (act_fn): SiLU()\n          )\n        )\n        (shared_expert): Qwen2MoeMLP(\n          (gate_proj): Linear(in_features=3584, out_features=20480, bias=False)\n          (up_proj): Linear(in_features=3584, out_features=20480, bias=False)\n          (down_proj): Linear(in_features=20480, out_features=3584, bias=False)\n          (act_fn): SiLU()\n        )\n        (shared_expert_gate): Linear(in_features=3584, out_features=1, bias=False)\n      )\n      (input_layernorm): Qwen2MoeRMSNorm()\n      (post_attention_layernorm): Qwen2MoeRMSNorm()\n    )\n  )\n  (norm): Qwen2MoeRMSNorm()\n)', 'per_layer_prefill_intput_threshold': '0'}

KQwen2MoeModel.forward
Args: ['KQwen2MoeModel']
Kwargs: {'input_ids': 'None', 'attention_mask': 'None', 'position_ids': 'None', 'past_key_values': 'StaticCache()', 'inputs_embeds': 'Tensor(shape=torch.Size([1, 23, 3584]), dtype=torch.bfloat16, device=cuda:0)', 'use_cache': 'True', 'output_attentions': 'False', 'output_hidden_states': 'False', 'output_router_logits': 'False', 'return_dict': 'False', 'cache_position': 'Tensor(shape=torch.Size([23]), dtype=torch.int32, device=cuda:0)'}

Qwen2MoeModel._update_causal_mask
Args: ['Qwen2MoeModel', 'None', 'Tensor(shape=torch.Size([1, 23, 3584]), dtype=torch.bfloat16, device=cuda:0)', 'Tensor(shape=torch.Size([23]), dtype=torch.int32, device=cuda:0)', 'StaticCache()', 'False']
Kwargs: {}

KQwen2MoeModel.forward
Args: ['KQwen2MoeModel']
Kwargs: {'input_ids': 'None', 'attention_mask': 'None', 'position_ids': 'Tensor(shape=torch.Size([1, 1]), dtype=torch.int32, device=cuda:0)', 'past_key_values': 'StaticCache()', 'inputs_embeds': 'Tensor(shape=torch.Size([1, 1, 3584]), dtype=torch.bfloat16, device=cuda:0)', 'use_cache': 'True', 'output_attentions': 'False', 'output_hidden_states': 'False', 'output_router_logits': 'False', 'return_dict': 'False', 'cache_position': 'Tensor(shape=torch.Size([1]), dtype=torch.int32, device=cuda:0)'}

Qwen2MoeModel._update_causal_mask
Args: ['Qwen2MoeModel', 'None', 'Tensor(shape=torch.Size([1, 1, 3584]), dtype=torch.bfloat16, device=cuda:0)', 'Tensor(shape=torch.Size([1]), dtype=torch.int32, device=cuda:0)', 'StaticCache()', 'False']
Kwargs: {}

KQwen2MoeModel.forward
Args: ['KQwen2MoeModel']
Kwargs: {'input_ids': 'None', 'attention_mask': 'None', 'position_ids': 'Tensor(shape=torch.Size([1, 1]), dtype=torch.int32, device=cuda:0)', 'past_key_values': 'StaticCache()', 'inputs_embeds': 'Tensor(shape=torch.Size([1, 1, 3584]), dtype=torch.bfloat16, device=cuda:0)', 'use_cache': 'True', 'output_attentions': 'False', 'output_hidden_states': 'False', 'output_router_logits': 'False', 'return_dict': 'False', 'cache_position': 'Tensor(shape=torch.Size([1]), dtype=torch.int32, device=cuda:0)'}

Qwen2MoeModel._update_causal_mask
Args: ['Qwen2MoeModel', 'None', 'Tensor(shape=torch.Size([1, 1, 3584]), dtype=torch.bfloat16, device=cuda:0)', 'Tensor(shape=torch.Size([1]), dtype=torch.int32, device=cuda:0)', 'StaticCache()', 'False']
Kwargs: {}

KQwen2MoeModel.forward
Args: ['KQwen2MoeModel']
Kwargs: {'input_ids': 'None', 'attention_mask': 'None', 'position_ids': 'None', 'past_key_values': 'StaticCache()', 'inputs_embeds': 'Tensor(shape=torch.Size([1, 20, 3584]), dtype=torch.bfloat16, device=cuda:0)', 'use_cache': 'True', 'output_attentions': 'False', 'output_hidden_states': 'False', 'output_router_logits': 'False', 'return_dict': 'False', 'cache_position': 'Tensor(shape=torch.Size([20]), dtype=torch.int32, device=cuda:0)'}

Qwen2MoeModel._update_causal_mask
Args: ['Qwen2MoeModel', 'None', 'Tensor(shape=torch.Size([1, 20, 3584]), dtype=torch.bfloat16, device=cuda:0)', 'Tensor(shape=torch.Size([20]), dtype=torch.int32, device=cuda:0)', 'StaticCache()', 'False']
Kwargs: {}

KQwen2MoeModel.forward
Args: ['KQwen2MoeModel']
Kwargs: {'input_ids': 'None', 'attention_mask': 'None', 'position_ids': 'Tensor(shape=torch.Size([1, 1]), dtype=torch.int32, device=cuda:0)', 'past_key_values': 'StaticCache()', 'inputs_embeds': 'Tensor(shape=torch.Size([1, 1, 3584]), dtype=torch.bfloat16, device=cuda:0)', 'use_cache': 'True', 'output_attentions': 'False', 'output_hidden_states': 'False', 'output_router_logits': 'False', 'return_dict': 'False', 'cache_position': 'Tensor(shape=torch.Size([1]), dtype=torch.int32, device=cuda:0)'}

Qwen2MoeModel._update_causal_mask
Args: ['Qwen2MoeModel', 'None', 'Tensor(shape=torch.Size([1, 1, 3584]), dtype=torch.bfloat16, device=cuda:0)', 'Tensor(shape=torch.Size([1]), dtype=torch.int32, device=cuda:0)', 'StaticCache()', 'False']
Kwargs: {}

KQwen2MoeModel.forward
Args: ['KQwen2MoeModel']
Kwargs: {'input_ids': 'None', 'attention_mask': 'None', 'position_ids': 'None', 'past_key_values': 'StaticCache()', 'inputs_embeds': 'Tensor(shape=torch.Size([1, 23, 3584]), dtype=torch.bfloat16, device=cuda:0)', 'use_cache': 'True', 'output_attentions': 'False', 'output_hidden_states': 'False', 'output_router_logits': 'False', 'return_dict': 'False', 'cache_position': 'Tensor(shape=torch.Size([23]), dtype=torch.int32, device=cuda:0)'}

Qwen2MoeModel._update_causal_mask
Args: ['Qwen2MoeModel', 'None', 'Tensor(shape=torch.Size([1, 23, 3584]), dtype=torch.bfloat16, device=cuda:0)', 'Tensor(shape=torch.Size([23]), dtype=torch.int32, device=cuda:0)', 'StaticCache()', 'False']
Kwargs: {}

KQwen2MoeModel.forward
Args: ['KQwen2MoeModel']
Kwargs: {'input_ids': 'None', 'attention_mask': 'None', 'position_ids': 'Tensor(shape=torch.Size([1, 1]), dtype=torch.int32, device=cuda:0)', 'past_key_values': 'StaticCache()', 'inputs_embeds': 'Tensor(shape=torch.Size([1, 1, 3584]), dtype=torch.bfloat16, device=cuda:0)', 'use_cache': 'True', 'output_attentions': 'False', 'output_hidden_states': 'False', 'output_router_logits': 'False', 'return_dict': 'False', 'cache_position': 'Tensor(shape=torch.Size([1]), dtype=torch.int32, device=cuda:0)'}

Qwen2MoeModel._update_causal_mask
Args: ['Qwen2MoeModel', 'None', 'Tensor(shape=torch.Size([1, 1, 3584]), dtype=torch.bfloat16, device=cuda:0)', 'Tensor(shape=torch.Size([1]), dtype=torch.int32, device=cuda:0)', 'StaticCache()', 'False']
Kwargs: {}

KQwen2MoeModel.forward
Args: ['KQwen2MoeModel']
Kwargs: {'input_ids': 'None', 'attention_mask': 'None', 'position_ids': 'None', 'past_key_values': 'StaticCache()', 'inputs_embeds': 'Tensor(shape=torch.Size([1, 162, 3584]), dtype=torch.bfloat16, device=cuda:0)', 'use_cache': 'True', 'output_attentions': 'False', 'output_hidden_states': 'False', 'output_router_logits': 'False', 'return_dict': 'False', 'cache_position': 'Tensor(shape=torch.Size([162]), dtype=torch.int32, device=cuda:0)'}

Qwen2MoeModel._update_causal_mask
Args: ['Qwen2MoeModel', 'None', 'Tensor(shape=torch.Size([1, 162, 3584]), dtype=torch.bfloat16, device=cuda:0)', 'Tensor(shape=torch.Size([162]), dtype=torch.int32, device=cuda:0)', 'StaticCache()', 'False']
Kwargs: {}

KQwen2MoeModel.forward
Args: ['KQwen2MoeModel']
Kwargs: {'input_ids': 'None', 'attention_mask': 'None', 'position_ids': 'Tensor(shape=torch.Size([1, 1]), dtype=torch.int32, device=cuda:0)', 'past_key_values': 'StaticCache()', 'inputs_embeds': 'Tensor(shape=torch.Size([1, 1, 3584]), dtype=torch.bfloat16, device=cuda:0)', 'use_cache': 'True', 'output_attentions': 'False', 'output_hidden_states': 'False', 'output_router_logits': 'False', 'return_dict': 'False', 'cache_position': 'Tensor(shape=torch.Size([1]), dtype=torch.int32, device=cuda:0)'}

Qwen2MoeModel._update_causal_mask
Args: ['Qwen2MoeModel', 'None', 'Tensor(shape=torch.Size([1, 1, 3584]), dtype=torch.bfloat16, device=cuda:0)', 'Tensor(shape=torch.Size([1]), dtype=torch.int32, device=cuda:0)', 'StaticCache()', 'False']
Kwargs: {}

KQwen2MoeModel.forward
Args: ['KQwen2MoeModel']
Kwargs: {'input_ids': 'None', 'attention_mask': 'None', 'position_ids': 'None', 'past_key_values': 'StaticCache()', 'inputs_embeds': 'Tensor(shape=torch.Size([1, 20, 3584]), dtype=torch.bfloat16, device=cuda:0)', 'use_cache': 'True', 'output_attentions': 'False', 'output_hidden_states': 'False', 'output_router_logits': 'False', 'return_dict': 'False', 'cache_position': 'Tensor(shape=torch.Size([20]), dtype=torch.int32, device=cuda:0)'}

Qwen2MoeModel._update_causal_mask
Args: ['Qwen2MoeModel', 'None', 'Tensor(shape=torch.Size([1, 20, 3584]), dtype=torch.bfloat16, device=cuda:0)', 'Tensor(shape=torch.Size([20]), dtype=torch.int32, device=cuda:0)', 'StaticCache()', 'False']
Kwargs: {}

KQwen2MoeModel.forward
Args: ['KQwen2MoeModel']
Kwargs: {'input_ids': 'None', 'attention_mask': 'None', 'position_ids': 'Tensor(shape=torch.Size([1, 1]), dtype=torch.int32, device=cuda:0)', 'past_key_values': 'StaticCache()', 'inputs_embeds': 'Tensor(shape=torch.Size([1, 1, 3584]), dtype=torch.bfloat16, device=cuda:0)', 'use_cache': 'True', 'output_attentions': 'False', 'output_hidden_states': 'False', 'output_router_logits': 'False', 'return_dict': 'False', 'cache_position': 'Tensor(shape=torch.Size([1]), dtype=torch.int32, device=cuda:0)'}

Qwen2MoeModel._update_causal_mask
Args: ['Qwen2MoeModel', 'None', 'Tensor(shape=torch.Size([1, 1, 3584]), dtype=torch.bfloat16, device=cuda:0)', 'Tensor(shape=torch.Size([1]), dtype=torch.int32, device=cuda:0)', 'StaticCache()', 'False']
Kwargs: {}

KQwen2MoeModel.forward
Args: ['KQwen2MoeModel']
Kwargs: {'input_ids': 'None', 'attention_mask': 'None', 'position_ids': 'None', 'past_key_values': 'StaticCache()', 'inputs_embeds': 'Tensor(shape=torch.Size([1, 57, 3584]), dtype=torch.bfloat16, device=cuda:0)', 'use_cache': 'True', 'output_attentions': 'False', 'output_hidden_states': 'False', 'output_router_logits': 'False', 'return_dict': 'False', 'cache_position': 'Tensor(shape=torch.Size([57]), dtype=torch.int32, device=cuda:0)'}

Qwen2MoeModel._update_causal_mask
Args: ['Qwen2MoeModel', 'None', 'Tensor(shape=torch.Size([1, 57, 3584]), dtype=torch.bfloat16, device=cuda:0)', 'Tensor(shape=torch.Size([57]), dtype=torch.int32, device=cuda:0)', 'StaticCache()', 'False']
Kwargs: {}

KQwen2MoeModel.forward
Args: ['KQwen2MoeModel']
Kwargs: {'input_ids': 'None', 'attention_mask': 'None', 'position_ids': 'Tensor(shape=torch.Size([1, 1]), dtype=torch.int32, device=cuda:0)', 'past_key_values': 'StaticCache()', 'inputs_embeds': 'Tensor(shape=torch.Size([1, 1, 3584]), dtype=torch.bfloat16, device=cuda:0)', 'use_cache': 'True', 'output_attentions': 'False', 'output_hidden_states': 'False', 'output_router_logits': 'False', 'return_dict': 'False', 'cache_position': 'Tensor(shape=torch.Size([1]), dtype=torch.int32, device=cuda:0)'}

Qwen2MoeModel._update_causal_mask
Args: ['Qwen2MoeModel', 'None', 'Tensor(shape=torch.Size([1, 1, 3584]), dtype=torch.bfloat16, device=cuda:0)', 'Tensor(shape=torch.Size([1]), dtype=torch.int32, device=cuda:0)', 'StaticCache()', 'False']
Kwargs: {}
