Log initialized at 2025-05-06 05:20:29.164681
------------------------------
[2025-05-06 05:20:30.323] GET IN Qwen2MoeForCausalLM: __init__
[2025-05-06 05:20:30.325] Qwen2MoeModel: __init__: padding_idx = None, vocab_size = 151936, hidden_size = 3584, num_hidden_layers = 28
[2025-05-06 05:20:30.331] Qwen2MoeSparseMoeBlock: __init__: num_experts = 64, top_k = 8, norm_topk_prob = 0
[2025-05-06 05:20:30.364] Qwen2MoeSparseMoeBlock: __init__: num_experts = 64, top_k = 8, norm_topk_prob = 0
[2025-05-06 05:20:30.397] Qwen2MoeSparseMoeBlock: __init__: num_experts = 64, top_k = 8, norm_topk_prob = 0
[2025-05-06 05:20:30.431] Qwen2MoeSparseMoeBlock: __init__: num_experts = 64, top_k = 8, norm_topk_prob = 0
[2025-05-06 05:20:30.463] Qwen2MoeSparseMoeBlock: __init__: num_experts = 64, top_k = 8, norm_topk_prob = 0
[2025-05-06 05:20:30.496] Qwen2MoeSparseMoeBlock: __init__: num_experts = 64, top_k = 8, norm_topk_prob = 0
[2025-05-06 05:20:30.529] Qwen2MoeSparseMoeBlock: __init__: num_experts = 64, top_k = 8, norm_topk_prob = 0
[2025-05-06 05:20:30.562] Qwen2MoeSparseMoeBlock: __init__: num_experts = 64, top_k = 8, norm_topk_prob = 0
[2025-05-06 05:20:30.594] Qwen2MoeSparseMoeBlock: __init__: num_experts = 64, top_k = 8, norm_topk_prob = 0
[2025-05-06 05:20:30.628] Qwen2MoeSparseMoeBlock: __init__: num_experts = 64, top_k = 8, norm_topk_prob = 0
[2025-05-06 05:20:30.661] Qwen2MoeSparseMoeBlock: __init__: num_experts = 64, top_k = 8, norm_topk_prob = 0
[2025-05-06 05:20:30.694] Qwen2MoeSparseMoeBlock: __init__: num_experts = 64, top_k = 8, norm_topk_prob = 0
[2025-05-06 05:20:30.727] Qwen2MoeSparseMoeBlock: __init__: num_experts = 64, top_k = 8, norm_topk_prob = 0
[2025-05-06 05:20:30.859] Qwen2MoeSparseMoeBlock: __init__: num_experts = 64, top_k = 8, norm_topk_prob = 0
[2025-05-06 05:20:30.891] Qwen2MoeSparseMoeBlock: __init__: num_experts = 64, top_k = 8, norm_topk_prob = 0
[2025-05-06 05:20:30.923] Qwen2MoeSparseMoeBlock: __init__: num_experts = 64, top_k = 8, norm_topk_prob = 0
[2025-05-06 05:20:30.956] Qwen2MoeSparseMoeBlock: __init__: num_experts = 64, top_k = 8, norm_topk_prob = 0
[2025-05-06 05:20:30.988] Qwen2MoeSparseMoeBlock: __init__: num_experts = 64, top_k = 8, norm_topk_prob = 0
[2025-05-06 05:20:31.020] Qwen2MoeSparseMoeBlock: __init__: num_experts = 64, top_k = 8, norm_topk_prob = 0
[2025-05-06 05:20:31.052] Qwen2MoeSparseMoeBlock: __init__: num_experts = 64, top_k = 8, norm_topk_prob = 0
[2025-05-06 05:20:31.084] Qwen2MoeSparseMoeBlock: __init__: num_experts = 64, top_k = 8, norm_topk_prob = 0
[2025-05-06 05:20:31.117] Qwen2MoeSparseMoeBlock: __init__: num_experts = 64, top_k = 8, norm_topk_prob = 0
[2025-05-06 05:20:31.149] Qwen2MoeSparseMoeBlock: __init__: num_experts = 64, top_k = 8, norm_topk_prob = 0
[2025-05-06 05:20:31.182] Qwen2MoeSparseMoeBlock: __init__: num_experts = 64, top_k = 8, norm_topk_prob = 0
[2025-05-06 05:20:31.214] Qwen2MoeSparseMoeBlock: __init__: num_experts = 64, top_k = 8, norm_topk_prob = 0
[2025-05-06 05:20:31.246] Qwen2MoeSparseMoeBlock: __init__: num_experts = 64, top_k = 8, norm_topk_prob = 0
[2025-05-06 05:20:31.279] Qwen2MoeSparseMoeBlock: __init__: num_experts = 64, top_k = 8, norm_topk_prob = 0
[2025-05-06 05:20:31.311] Qwen2MoeSparseMoeBlock: __init__: num_experts = 64, top_k = 8, norm_topk_prob = 0
[2025-05-06 05:20:31.361] layers: ModuleList(
  (0-27): 28 x Qwen2MoeDecoderLayer(
    (self_attn): Qwen2MoeFlashAttention2(
      (q_proj): Linear(in_features=3584, out_features=3584, bias=True)
      (k_proj): Linear(in_features=3584, out_features=512, bias=True)
      (v_proj): Linear(in_features=3584, out_features=512, bias=True)
      (o_proj): Linear(in_features=3584, out_features=3584, bias=False)
      (rotary_emb): Qwen2MoeRotaryEmbedding()
    )
    (mlp): Qwen2MoeSparseMoeBlock(
      (gate): Linear(in_features=3584, out_features=64, bias=False)
      (experts): ModuleList(
        (0-63): 64 x Qwen2MoeMLP(
          (gate_proj): Linear(in_features=3584, out_features=2560, bias=False)
          (up_proj): Linear(in_features=3584, out_features=2560, bias=False)
          (down_proj): Linear(in_features=2560, out_features=3584, bias=False)
          (act_fn): SiLU()
        )
      )
      (shared_expert): Qwen2MoeMLP(
        (gate_proj): Linear(in_features=3584, out_features=20480, bias=False)
        (up_proj): Linear(in_features=3584, out_features=20480, bias=False)
        (down_proj): Linear(in_features=20480, out_features=3584, bias=False)
        (act_fn): SiLU()
      )
      (shared_expert_gate): Linear(in_features=3584, out_features=1, bias=False)
    )
    (input_layernorm): Qwen2MoeRMSNorm()
    (post_attention_layernorm): Qwen2MoeRMSNorm()
  )
)
